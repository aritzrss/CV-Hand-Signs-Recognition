# Clasificador de Gestos EstÃ¡ticos LSE (Lengua de Signos EspaÃ±ola)

Proyecto de Computer Vision para clasificar gestos estÃ¡ticos de la mano (letras A, B, C, D, E del LSE) usando MediaPipe y Machine Learning.

## ğŸ“‹ CaracterÃ­sticas

- **Captura de datos**: Interfaz intuitiva para recolectar imÃ¡genes de gestos
- **ExtracciÃ³n de caracterÃ­sticas**: Uso de MediaPipe para detectar 21 landmarks de la mano
- **MÃºltiples modelos**: ComparaciÃ³n automÃ¡tica de Random Forest, SVM, KNN y Redes Neuronales
- **Reconocimiento en tiempo real**: ClasificaciÃ³n de gestos en vivo con webcam
- **Visualizaciones**: GrÃ¡ficos de rendimiento y matriz de confusiÃ³n

## ğŸ¯ Gestos Soportados

- **A**: PuÃ±o cerrado
- **B**: Mano plana, dedos juntos
- **C**: Mano en forma de C
- **D**: Ãndice levantado, otros dedos doblados
- **E**: Todos los dedos doblados hacia la palma

## ğŸš€ InstalaciÃ³n

### Prerrequisitos

- Python 3.8 o superior
- Webcam funcional
- Sistema operativo: Windows, macOS o Linux

### Pasos de instalaciÃ³n

1. **Clonar o descargar el proyecto**

```bash
# Si tienes el cÃ³digo en un repositorio
git clone <tu-repositorio>
cd clasificador-gestos-lse
```

2. **Crear entorno virtual (recomendado)**

```bash
python -m venv venv

# En Windows:
venv\Scripts\activate

# En macOS/Linux:
source venv/bin/activate
```

3. **Instalar dependencias**

```bash
pip install -r requirements.txt
```

## ğŸ“– Uso

### Paso 1: Capturar Datos

Ejecuta el script de captura para recolectar imÃ¡genes de tus gestos:

```bash
python capture_gestures.py
```

**Instrucciones:**
- Muestra el gesto indicado en pantalla
- Presiona **ESPACIO** para capturar una muestra
- El programa capturarÃ¡ 100 muestras por gesto automÃ¡ticamente
- Presiona **'n'** para saltar al siguiente gesto manualmente
- Presiona **'q'** para salir

**Consejos para mejores resultados:**
- MantÃ©n la mano en el centro del encuadre
- VarÃ­a ligeramente la posiciÃ³n y Ã¡ngulo de la mano
- AsegÃºrate de tener buena iluminaciÃ³n
- Usa un fondo simple y sin distracciones

### Paso 2: Entrenar el Modelo

Una vez capturados los datos, entrena el clasificador:

```bash
python train_model.py
```

El script:
1. Carga los datos capturados
2. Entrena 4 modelos diferentes (Random Forest, SVM, KNN, MLP)
3. EvalÃºa cada modelo con validaciÃ³n cruzada
4. Selecciona automÃ¡ticamente el mejor modelo
5. Genera visualizaciones y reportes
6. Guarda el modelo entrenado

**Archivos generados:**
- `data/gesture_model_latest.pkl` - Modelo entrenado (el mÃ¡s importante)
- `data/model_comparison.png` - ComparaciÃ³n de modelos
- `data/confusion_matrix.png` - Matriz de confusiÃ³n
- `data/classification_report.txt` - Reporte detallado

### Paso 3: Reconocimiento en Tiempo Real

Usa el modelo entrenado para reconocer gestos en vivo:

```bash
python real_time_recognition.py
```

**Controles:**
- **'q'** - Salir de la aplicaciÃ³n
- **'r'** - Reiniciar el buffer de predicciones (Ãºtil si hay errores)
- **'h'** - Mostrar/ocultar los landmarks de la mano

**InformaciÃ³n en pantalla:**
- Gesto predicho con color distintivo
- Nivel de confianza de la predicciÃ³n
- FPS del sistema
- Probabilidades de cada clase (panel derecho)

## ğŸ§  Arquitectura del Sistema

### 1. Captura de Datos (`capture_gestures.py`)

```
Webcam â†’ MediaPipe â†’ Landmarks (21 puntos, x/y/z) â†’ NormalizaciÃ³n â†’ Dataset
```

- Detecta 21 puntos clave de la mano (landmarks)
- Normaliza las coordenadas respecto a la muÃ±eca
- Escala por la distancia muÃ±eca-dedo medio
- Genera vectores de 63 caracterÃ­sticas (21 puntos Ã— 3 coordenadas)

### 2. Entrenamiento (`train_model.py`)

```
Dataset â†’ Split (80/20) â†’ Modelos ML â†’ ValidaciÃ³n â†’ Mejor Modelo â†’ .pkl
```

**Modelos evaluados:**
- **Random Forest**: Ensemble de Ã¡rboles de decisiÃ³n
- **SVM**: Support Vector Machine con kernel RBF
- **KNN**: K-Nearest Neighbors (k=5)
- **MLP**: Red neuronal (128-64-32 neuronas)

**MÃ©tricas:**
- Accuracy en conjunto de test
- ValidaciÃ³n cruzada (5-fold)
- Precision, Recall, F1-Score por clase

### 3. Reconocimiento (`real_time_recognition.py`)

```
Webcam â†’ MediaPipe â†’ NormalizaciÃ³n â†’ Modelo â†’ Suavizado â†’ PredicciÃ³n Final
```

- Procesamiento en tiempo real (~30 FPS)
- Buffer de predicciones para suavizado (reduce falsos positivos)
- VisualizaciÃ³n de confianza y probabilidades

## ğŸ“Š Estructura de Archivos

```
clasificador-gestos-lse/
â”‚
â”œâ”€â”€ capture_gestures.py          # Script de captura de datos
â”œâ”€â”€ train_model.py                # Script de entrenamiento
â”œâ”€â”€ real_time_recognition.py     # Script de reconocimiento en vivo
â”œâ”€â”€ requirements.txt              # Dependencias
â”œâ”€â”€ README.md                     # Esta documentaciÃ³n
â”‚
â””â”€â”€ data/                         # Carpeta generada automÃ¡ticamente
    â”œâ”€â”€ gestures_data_*.npy      # Datos de entrenamiento
    â”œâ”€â”€ gestures_labels_*.npy    # Etiquetas
    â”œâ”€â”€ metadata_*.json          # InformaciÃ³n de captura
    â”œâ”€â”€ gesture_model_latest.pkl # Modelo entrenado (importante!)
    â”œâ”€â”€ model_comparison.png     # VisualizaciÃ³n de modelos
    â”œâ”€â”€ confusion_matrix.png     # Matriz de confusiÃ³n
    â””â”€â”€ classification_report.txt # Reporte detallado
```

## ğŸ”§ SoluciÃ³n de Problemas

### La webcam no funciona

```python
# En real_time_recognition.py, cambia el Ã­ndice de la cÃ¡mara:
cap = cv2.VideoCapture(0)  # Prueba con 1, 2, etc.
```

### Baja precisiÃ³n del modelo

1. Captura mÃ¡s datos (aumenta `samples_per_gesture` en `capture_gestures.py`)
2. Mejora la calidad de las capturas (iluminaciÃ³n, fondo limpio)
3. AsegÃºrate de hacer los gestos de forma consistente

### El reconocimiento es muy sensible

En `real_time_recognition.py`, aumenta el tamaÃ±o del buffer:

```python
self.prediction_buffer = deque(maxlen=15)  # Por defecto es 10
```

### Errores de dependencias

```bash
# Reinstalar todas las dependencias
pip install --upgrade -r requirements.txt
```

## ğŸ¨ PersonalizaciÃ³n

### AÃ±adir mÃ¡s gestos

1. Modifica la lista en `capture_gestures.py`:
```python
self.gestures = ['A', 'B', 'C', 'D', 'E', 'F', 'G']  # AÃ±ade mÃ¡s letras
```

2. AÃ±ade colores en `real_time_recognition.py`:
```python
self.colors = {
    'A': (255, 100, 100),
    'F': (100, 255, 255),  # AÃ±ade color para F
    'G': (255, 150, 100),  # AÃ±ade color para G
}
```

### Cambiar el nÃºmero de muestras

En `capture_gestures.py`:
```python
self.samples_per_gesture = 150  # Por defecto es 100
```

### Ajustar confianza de detecciÃ³n

En `capture_gestures.py` o `real_time_recognition.py`:
```python
self.hands = self.mp_hands.Hands(
    min_detection_confidence=0.8,  # Aumenta para mÃ¡s precisiÃ³n
    min_tracking_confidence=0.7
)
```

## ğŸ“ˆ Resultados Esperados

Con 100 muestras por gesto:
- **Accuracy esperada**: 90-98%
- **FPS**: 25-35 en hardware moderno
- **Tiempo de entrenamiento**: 10-30 segundos

## ğŸ¤ Contribuciones

Â¿Mejoras o sugerencias? Â¡Son bienvenidas!

## ğŸ“ Notas Adicionales

- Los modelos se guardan automÃ¡ticamente despuÃ©s del entrenamiento
- Puedes reentrenar en cualquier momento ejecutando `train_model.py`
- Los datos antiguos no se sobrescriben, se crean nuevos archivos con timestamp
- Para mejores resultados, captura datos con diferentes condiciones de iluminaciÃ³n

## ğŸ” Referencias

- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html)
- [OpenCV Python](https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html)
- [Scikit-learn](https://scikit-learn.org/stable/)
- [Lengua de Signos EspaÃ±ola](https://www.cnse.es/)

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible bajo la licencia MIT.

---

**Â¡Disfruta clasificando gestos! ğŸ¤Ÿ**